{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83135421",
   "metadata": {},
   "source": [
    "# 8.4\tMit vortrainierten Worteinbettungen arbeiten (fastText)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e574fc9",
   "metadata": {},
   "source": [
    "### 01 - Heidegger-Buchtext als String laden und tokenisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0028b4",
   "metadata": {},
   "source": [
    "#### a) Text als String laden und mit der Funktion *nltk.tokenize.word_tokenize* tokenisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28366a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('denn offenbar seid ihr doch schon\\nlange mit dem vertraut, was ihr eigentlich meint, wenn ihr den\\naus',\n",
       " ['denn',\n",
       "  'offenbar',\n",
       "  'seid',\n",
       "  'ihr',\n",
       "  'doch',\n",
       "  'schon',\n",
       "  'lange',\n",
       "  'mit',\n",
       "  'dem',\n",
       "  'vertraut'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from os.path import join\n",
    "\n",
    "path = r'..\\Data'\n",
    "\n",
    "with open(join(path, 'Heidegger_bereinigt.txt'), 'r', \n",
    "           encoding='latin-1') as doc:\n",
    "     text = doc.read()\n",
    "     text_words = word_tokenize(text)\n",
    "\n",
    "text[:100], text_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c26eb5",
   "metadata": {},
   "source": [
    "### 02 - Den *fastText*-Vektorraum laden\n",
    "**Hinweis**: Da die *fastText*-Datei mit den Vektoren *wiki.de.align.vec* zu groß ist, befindest sich diese nicht ins Repository gela. Wenn Sie den Code ausführen möchten, gehen Sie wie folgt vor:\n",
    "- Laden Sie zunächst den fastText-Vektorraum von folgender Seite: https://fasttext.cc/docs/en/aligned-vectors.html (Es handelt sich um den *aligned vector German (Text)*. Direkter Download Link: https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec [Zuletzt geprüft am 10.01.2022]\n",
    "- Platzieren Sie die Datei im Repository im Ordner Data (oder an einem anderen Ort auf ihrem Rechner und passen Sie die Variable *path* entsprechend an.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda6ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_data(filepath: str):\n",
    "    with open(filepath, 'r', encoding='utf-8') as doc:\n",
    "        next(doc)\n",
    "        word_vec = {}\n",
    "        for row in doc:\n",
    "            row = row.split(' ')\n",
    "            word = row[0]\n",
    "            vec = np.array([float(v) for v in row[1:]])\n",
    "            word_vec[word] = vec\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "949fc496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2275233, (300,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'..\\Data'\n",
    "\n",
    "word_vec = load_vector_data(join(path, 'wiki.de.align.vec'))\n",
    "len(word_vec), word_vec['angst'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0d50f8",
   "metadata": {},
   "source": [
    "### 03 - Relevante Wörter für die Aufgabe extrahieren\n",
    "Wörter aus dem Heidegger-Text aus dem geladenen fastText-Vektorraum extrahieren und in *word_vec_task* speichern. Gleichzeitig ein Dicitonary anlegen, das zum Nachschlagen der Wörter geeignet ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bff16491",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_word_vectors(words: list, word_vec: dict ):\n",
    "    word_vect_index = {}\n",
    "    word_vect_task = [np.zeros(300)]\n",
    "    count = 1\n",
    "    for word in words:\n",
    "        if word in word_vec:\n",
    "            word_vect_index[word] = count\n",
    "            word_vect_task.append(word_vec[word])\n",
    "            count += 1\n",
    "        else:\n",
    "            word_vect_index[word] = 0\n",
    "    return word_vect_index, word_vect_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb6c8e",
   "metadata": {},
   "source": [
    "#### Eine Liste mit jeweils einem Exemplar der Wörter aus dem Heidegger-Text erzeugen (Grundlage ist die Liste *text_words*) und damit die Funktion zusammen mit dem fastText-Vektorraum füttern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01f4998f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12653, 10184)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_text = [word for word in set(text_words)]\n",
    "words_in_text.sort()\n",
    "word_vect_index, word_vect_task = selected_word_vectors(\n",
    "                                words_in_text, word_vec)\n",
    "len(word_vect_index), len(word_vect_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766537d4",
   "metadata": {},
   "source": [
    "#### Den fastText-Vektorraum wieder aus dem Arbeitsspeicher löschen, um Platz zu schaffen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a9b733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f236dc3a",
   "metadata": {},
   "source": [
    "### 04 - Den Text in Sequenzen zerlegen und die Features und Targets encodieren \n",
    "In diesem Fall benötigen wir unterschiedliche Dictionaries für die Encodierung der Features und der Zielvariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b15667dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequences( text: list, \n",
    "                       words_index_x: dict, \n",
    "                       words_index_y: dict, \n",
    "                       len_seq: int):\n",
    "   data_X = []\n",
    "   data_y = []\n",
    "   for i in range(len(text)-(len_seq+1)):\n",
    "      X = text[i:i+len_seq]\n",
    "      y = text[i+len_seq]\n",
    "      X = [words_index_x[word] for word in X]\n",
    "      y = words_index_y[y]\n",
    "      data_X.append(X)\n",
    "      data_y.append(y)\n",
    "   return np.array(data_X), np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2247420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2027, 6124, 6997, 4745, 2155, 6892, 5239, 5626, 2005, 8733,    7,\n",
       "         9212, 4745, 2347, 5549,    7, 9342, 4745, 2015,  797],\n",
       "        [6124, 6997, 4745, 2155, 6892, 5239, 5626, 2005, 8733,    7, 9212,\n",
       "         4745, 2347, 5549,    7, 9342, 4745, 2015,  797, 6999]]),\n",
       " array([8505, 4334]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_word_index = dict([(word, idx) for idx, word \n",
    "                      in enumerate(words_in_text)])\n",
    "\n",
    "X, y = text_to_sequences( text_words, \n",
    "                          word_vect_index,\n",
    "                          y_word_index, \n",
    "                          len_seq=20)\n",
    "\n",
    "X[:2], y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364dcb56",
   "metadata": {},
   "source": [
    "#### Einteilung in Trainings- und Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2960a727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((157311, 20), (17480, 20), (157311,), (17480,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                     test_size=.1, \n",
    "                                     random_state=12)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06256524",
   "metadata": {},
   "source": [
    "### 05 - Neuronales Netz für Aufgabe zusammenstellen\n",
    "***Hinweis:*** Da wir die Embedding-Gewichte später durch die aus *fastText* extrahierten Werte ersetzen, orientieren wir uns bei der Einstellung des Embedding-Layers an *word_vect_task*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7950528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 300)           3055200   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 300)               541800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12653)             3808553   \n",
      "=================================================================\n",
      "Total params: 7,405,553\n",
      "Trainable params: 7,405,553\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GRU, Dropout\n",
    "\n",
    "# Zunächst word_vect_task in ein Numpy-Array verwandeln (ist bist jetzt eine Liste)\n",
    "word_vect_task = np.array(word_vect_task)\n",
    "\n",
    "# Relevante Einstellungen abfragen und speichern\n",
    "len_seq = len(X[0])\n",
    "num_words_x = word_vect_task.shape[0]\n",
    "num_vects = word_vect_task.shape[1]\n",
    "num_words_y = len(y_word_index)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words_x, \n",
    "                    output_dim=num_vects, \n",
    "                    input_length=len_seq,\n",
    "                    mask_zero=True))\n",
    "model.add(GRU(units=num_vects))\n",
    "model.add(Dense(units=num_words_y, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile( loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28de60e",
   "metadata": {},
   "source": [
    "### 06 - Die Gewichte des Embedding-Layers durch die fastText-Gewichte ersetzen\n",
    "Zuästzlich setzen wir die Gewichte nach dem Austausch auf nicht-trainierbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e70198b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 300)           3055200   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 300)               541800    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12653)             3808553   \n",
      "=================================================================\n",
      "Total params: 7,405,553\n",
      "Trainable params: 4,350,353\n",
      "Non-trainable params: 3,055,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.get_layer(index=0).set_weights([word_vect_task])\n",
    "model.get_layer(index=0).trainable = False\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0b6f4",
   "metadata": {},
   "source": [
    "### 07 - Trainingsprozess beginnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04fc8d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1229/1229 [==============================] - 593s 482ms/step - loss: 6.1801 - accuracy: 0.1015 - val_loss: 5.6022 - val_accuracy: 0.1466\n",
      "Epoch 2/100\n",
      "1229/1229 [==============================] - 1720s 1s/step - loss: 5.1539 - accuracy: 0.1639 - val_loss: 5.3605 - val_accuracy: 0.1709\n",
      "Epoch 3/100\n",
      "1229/1229 [==============================] - 412s 335ms/step - loss: 4.5724 - accuracy: 0.1985 - val_loss: 5.3497 - val_accuracy: 0.1781\n",
      "Epoch 4/100\n",
      "1229/1229 [==============================] - 405s 330ms/step - loss: 4.0044 - accuracy: 0.2326 - val_loss: 5.4302 - val_accuracy: 0.1800\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint( filepath='heidegger_fasttext.h5',\n",
    "                              save_best_only=False)\n",
    "stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpoint, stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e2044",
   "metadata": {},
   "source": [
    "### 08 - Schätzungen erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e79e93df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrases(model, \n",
    "                    x_words_index: dict,\n",
    "                    y_index_words: dict,\n",
    "                    starter: str,\n",
    "                    len_seq = 20, \n",
    "                    num_words=100):\n",
    "    starter = word_tokenize(starter.lower())\n",
    "    print(' '.join(starter))\n",
    "    starter = np.array([x_words_index[word] for word \n",
    "                        in starter]).reshape(1, -1)\n",
    "    for i in range(num_words):\n",
    "        word_probs = model.predict(starter)[0]\n",
    "        word_pred = np.argmax(word_probs)\n",
    "        word = y_index_words[word_pred]\n",
    "        print(word, end=' ')\n",
    "        starter = starter[0].tolist()[1:]\n",
    "        starter.append(x_words_index[word])\n",
    "        starter = np.array(starter).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de6b6bb",
   "metadata": {},
   "source": [
    "#### Zunächst ein Reverse-Index zum Nachschlagen der Targets erzeugen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5cce02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('!', 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_index_word = dict([(i, word) for word, i in y_word_index.items()])\n",
    "y_index_word[0], y_word_index['!']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279f77b",
   "metadata": {},
   "source": [
    "#### Dann Schätzungen produzieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "438e5447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wie unterscheidet sich das , wovor die angst sich ängstet , von dem , wovor die furcht sich fürchtet ?\n",
      "die furcht bringt das dasein als angerufenes nicht nur als vorhandenes , sondern sie ist die möglichkeit der existenz . die entschlossenheit ist die möglichkeit der existenz , das heißt die möglichkeit der zukunft . die entschlossenheit bringt das dasein als das in der welt sein , das heißt die furcht als die gewesenheit . die furcht bringt die furcht als in der welt sein . die analyse der alltäglichkeit ist die ständigkeit des strukturganzen strukturganzen in der welt seins . die fundierende des seins des daseins ist die existenziale bedingung der möglichkeit der existenz , die wir als die "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('heidegger_fasttext.h5')\n",
    "\n",
    "starter = '''wie unterscheidet sich das, wovor die angst sich ängstet, von dem, wovor die furcht sich fürchtet?'''\n",
    "generate_phrases(model, \n",
    "                word_vect_index, \n",
    "                y_index_word, \n",
    "                starter,\n",
    "                num_words=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
