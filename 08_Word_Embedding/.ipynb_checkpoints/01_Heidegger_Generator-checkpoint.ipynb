{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e427d9b5",
   "metadata": {},
   "source": [
    "# 8.3.2\tDer Heidegger-Algorithmus: <br>Ein generatives Modell zur Erzeugung von Texten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35947b18",
   "metadata": {},
   "source": [
    "### 01 - Heidegger-Buchtext als String laden und vorverarbeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a716aaef",
   "metadata": {},
   "source": [
    "#### a) Text als String laden und mit der Funktion *nltk.tokenize.word_tokenize* tokenisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea64384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('denn offenbar seid ihr doch schon\\nlange mit dem vertraut, was ihr eigentlich meint, wenn ihr den\\naus',\n",
       " ['denn',\n",
       "  'offenbar',\n",
       "  'seid',\n",
       "  'ihr',\n",
       "  'doch',\n",
       "  'schon',\n",
       "  'lange',\n",
       "  'mit',\n",
       "  'dem',\n",
       "  'vertraut'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from os.path import join\n",
    "\n",
    "path = r'..\\Data'\n",
    "\n",
    "with open(join(path, 'Heidegger_bereinigt.txt'), 'r', \n",
    "           encoding='latin-1') as doc:\n",
    "     text = doc.read()\n",
    "     text_words = word_tokenize(text)\n",
    "\n",
    "text[:100], text_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283cee8",
   "metadata": {},
   "source": [
    "#### b) Dictionary erzeugen, das für jedes enthaltene Wort aus dem Text eine Integer vorhält (*word_index* und Reverse-Dictionary: *index_word*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "374b5323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4488, 'gehen')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = [word for word in set(text_words)]\n",
    "word_index.sort()\n",
    "\n",
    "word_index = dict([(word, i) for i, word \n",
    "                             in enumerate(word_index)])\n",
    "index_word = dict([(index, word) for word, index \n",
    "                                 in word_index.items()])\n",
    "\n",
    "word_index['gehen'], index_word[4488]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c18175",
   "metadata": {},
   "source": [
    "#### c) Text in Sequenzen mit fixer Länge zerschneiden und mit Hilfe des Dictionarys *word_index* die Wörter als Integer codieren (Word-IDs). Gleichzeitig das jeweils nachfolgende Wort als Zielvariable festlegen und mit *word_index* encodieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c35086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequences(text: list, words_index: dict, len_seq: int):\n",
    "    data_X = []\n",
    "    data_y = []\n",
    "    for i in range(len(text)-(len_seq+1)):\n",
    "        X = text[i:i+len_seq]\n",
    "        y = text[i+len_seq]\n",
    "        X = [words_index[word] for word in X]\n",
    "        y = words_index[y]\n",
    "        data_X.append(X)\n",
    "        data_y.append(y)\n",
    "    return np.array(data_X), np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0407ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2427,  7400,  8503,  5707,  2582,  8380,  6303,  6734,  2404,\n",
       "         10751,     7, 11352,  5707,  2814,  6648,     7, 11526,  5707,\n",
       "          2414,   941],\n",
       "        [ 7400,  8503,  5707,  2582,  8380,  6303,  6734,  2404, 10751,\n",
       "             7, 11352,  5707,  2814,  6648,     7, 11526,  5707,  2414,\n",
       "           941,  8505]]),\n",
       " array([8505, 4334]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Funktion mit Sequenzlänge = 20 aufrufen\n",
    "X, y = text_to_sequences(text_words, word_index, len_seq=20)\n",
    "X[:2], y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac79b92",
   "metadata": {},
   "source": [
    "#### d) Daten in Trainings- und Testdaten separieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "923a34fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((157311, 20), (17480, 20), (157311,), (17480,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                        test_size=.1, random_state=13)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431fbde",
   "metadata": {},
   "source": [
    "## 02 - Neuronales Netz zusammenstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1db58c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 50)            632650    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 50)                15300     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12653)             645303    \n",
      "=================================================================\n",
      "Total params: 1,293,253\n",
      "Trainable params: 1,293,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import  Dense, Embedding, GRU, Dropout\n",
    "\n",
    "len_seq = len(X[0])\n",
    "num_words = len(word_index)\n",
    "num_vects = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words, \n",
    "                    output_dim=num_vects, input_length=len_seq))\n",
    "model.add(GRU(units=num_vects))\n",
    "model.add(Dense(units=num_words, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(  loss='sparse_categorical_crossentropy', \n",
    "                optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab855f98",
   "metadata": {},
   "source": [
    "### 03 - Training durchführen\n",
    "***Hinweis:*** Im Vergleich zum Buch ist *patience* hier auf einen geringeren Wert eingestellt, so dass der Anlernprozess schneller abläuft. Dadurch kommen unter Umständen andere Ergebnisse als im Buch verzeichnet zustande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ecbc695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1229/1229 [==============================] - 72s 59ms/step - loss: 6.4296 - accuracy: 0.0702 - val_loss: 6.0218 - val_accuracy: 0.1042\n",
      "Epoch 2/50\n",
      "1229/1229 [==============================] - 79s 64ms/step - loss: 5.7147 - accuracy: 0.1181 - val_loss: 5.7404 - val_accuracy: 0.1244\n",
      "Epoch 3/50\n",
      "1229/1229 [==============================] - 78s 63ms/step - loss: 5.3967 - accuracy: 0.1421 - val_loss: 5.6097 - val_accuracy: 0.1417\n",
      "Epoch 4/50\n",
      "1229/1229 [==============================] - 75s 61ms/step - loss: 5.1600 - accuracy: 0.1603 - val_loss: 5.5311 - val_accuracy: 0.1515\n",
      "Epoch 5/50\n",
      "1229/1229 [==============================] - 75s 61ms/step - loss: 4.9731 - accuracy: 0.1756 - val_loss: 5.5044 - val_accuracy: 0.1562\n",
      "Epoch 6/50\n",
      "1229/1229 [==============================] - 74s 60ms/step - loss: 4.8177 - accuracy: 0.1890 - val_loss: 5.4989 - val_accuracy: 0.1585\n",
      "Epoch 7/50\n",
      "1229/1229 [==============================] - 76s 62ms/step - loss: 4.6849 - accuracy: 0.1990 - val_loss: 5.4985 - val_accuracy: 0.1621\n",
      "Epoch 8/50\n",
      "1229/1229 [==============================] - 74s 60ms/step - loss: 4.5662 - accuracy: 0.2072 - val_loss: 5.5148 - val_accuracy: 0.1645\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint( filepath='heidegger_algo.h5',\n",
    "                              save_best_only=False)\n",
    "stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=50, \n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[stopping, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40843df",
   "metadata": {},
   "source": [
    "### 04 - Texte produzieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663ad53e",
   "metadata": {},
   "source": [
    "#### Funktion anlegen, die das Modell mit Eingaben füttert und Ausgaben abfängt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70743f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_phrases( model, \n",
    "                      words_index: dict,\n",
    "                      index_words: dict,\n",
    "                      starter: str, \n",
    "                      num_words=100):\n",
    "    print(starter.lower())\n",
    "    starter = word_tokenize(starter.lower())\n",
    "    starter = np.array([words_index[word] for word in starter]).reshape(1, -1)\n",
    "    for i in range(num_words):\n",
    "        word_probs = model.predict(starter)[0]\n",
    "        word_pred = np.argmax(word_probs)\n",
    "        print(index_words[word_pred], end=' ')\n",
    "        starter = starter[0].tolist()[1:]\n",
    "        starter.append(word_pred)\n",
    "        starter = np.array(starter).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21074e8",
   "metadata": {},
   "source": [
    "#### Testlauf durchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43457503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wie unterscheidet sich das, wovor die angst sich ängstet, von dem, wovor die furcht sich fürchtet?\n",
      "die existenziale analytik des daseins ist die existenziale interpretation des daseins . die frage nach dem sinn der welt , die das dasein selbst ist , daß es sich in der welt sein . das dasein ist das dasein selbst . das dasein ist das dasein selbst . das dasein ist das dasein selbst . das dasein ist das dasein selbst . das dasein ist das dasein selbst . das dasein ist das dasein selbst . das dasein ist das dasein selbst . das dasein ist das dasein selbst . das dasein ist das dasein selbst . das dasein ist "
     ]
    }
   ],
   "source": [
    "starter = '''wie unterscheidet sich das, wovor die angst sich ängstet, von dem, wovor die furcht sich fürchtet?'''\n",
    "generate_phrases(model, \n",
    "                word_index, \n",
    "                index_word, \n",
    "                starter,\n",
    "                num_words=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cbb4b7",
   "metadata": {},
   "source": [
    "### 05 - Synonyme Wörter identifizieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa76901",
   "metadata": {},
   "source": [
    "#### a) Zunächst die Gewichte aus dem angelernten Modell extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d45f6b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12653, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('heidegger_algo.h5')\n",
    "vectors = model.get_layer(index=0).get_weights()[0]\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8c713",
   "metadata": {},
   "source": [
    "#### b) Jetzt die Cosinus-Ähnlichkeiten berechnen und zur Veranschaulichung in einen DataFrame umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "234c2ef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>#</th>\n",
       "      <th>%</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>...</th>\n",
       "      <th>üoxepov</th>\n",
       "      <th>üpd</th>\n",
       "      <th>üttokeipevov</th>\n",
       "      <th>üürities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.391</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.352</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.530</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.040</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.177</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>-0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>%</th>\n",
       "      <td>0.391</td>\n",
       "      <td>0.254</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.610</td>\n",
       "      <td>-0.196</td>\n",
       "      <td>-0.239</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&amp;</th>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.380</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.325</td>\n",
       "      <td>0.121</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>(</th>\n",
       "      <td>0.669</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.469</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 12653 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       !      #      %      &  ...  üoxepov    üpd  üttokeipevov  üürities\n",
       "!  1.000  0.530  0.391 -0.125  ...   -0.352 -0.142         0.258     0.390\n",
       "#  0.530  1.000  0.254  0.040  ...   -0.177  0.046        -0.072    -0.037\n",
       "%  0.391  0.254  1.000  0.380  ...   -0.610 -0.196        -0.239     0.056\n",
       "& -0.125  0.040  0.380  1.000  ...   -0.325  0.121        -0.116    -0.390\n",
       "(  0.669  0.601  0.469 -0.130  ...   -0.337 -0.032         0.132     0.091\n",
       "\n",
       "[5 rows x 12653 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', 9)\n",
    "\n",
    "cos_sim = cosine_similarity(vectors)\n",
    "cos_sim = pd.DataFrame(cos_sim)\n",
    "cos_sim.index = word_index.keys()\n",
    "cos_sim.columns = word_index.keys()\n",
    "cos_sim.head().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e90092",
   "metadata": {},
   "source": [
    "#### c) Nächste Nachbarn eines Wortes auf Grundlage der Similiarity-Matrix bestimmen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfc8ea43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similiar_words(word, matrix, word_index, index_word, num_words=5):    \n",
    "    idx = np.argsort(matrix[word_index[word]])\n",
    "    idx = idx[:-(num_words+1): -1]\n",
    "    sim_words = [(index_word[ix], matrix[word_index[word]][ix].round(3)) for ix in idx]\n",
    "    return dict(sim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "731e47da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ist': 1.0, 'wird': 0.794, 'hat': 0.776, 'kann': 0.725, 'ob': 0.679}\n",
      "{'gehen': 1.0, 'vorrufen': 0.851, 'stürzt': 0.841, 'sichverstehen': 0.821, 'absehen': 0.806}\n",
      "{'vorrang': 1.0, 'unbestimmtheit': 0.901, 'modifikation': 0.869, 'entdecktbeit': 0.863, 'weltmäßigkeit': 0.863}\n"
     ]
    }
   ],
   "source": [
    "words = ['ist', 'gehen', 'vorrang']\n",
    "\n",
    "for word in words:   \n",
    "    msw = most_similiar_words(word, cos_sim.values, \n",
    "                    word_index, index_word)\n",
    "    print(msw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
