{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83135421",
   "metadata": {},
   "source": [
    "# 8.4.3\tDen *fastText*-Vektorraum um unbekannte Wörter erweitern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e574fc9",
   "metadata": {},
   "source": [
    "### 01 - Heidegger-Buchtext als String laden und tokenisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0028b4",
   "metadata": {},
   "source": [
    "#### a) Text als String laden und mit der Funktion *nltk.tokenize.word_tokenize* tokenisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28366a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('denn offenbar seid ihr doch schon\\nlange mit dem vertraut, was ihr eigentlich meint, wenn ihr den\\naus',\n",
       " ['denn',\n",
       "  'offenbar',\n",
       "  'seid',\n",
       "  'ihr',\n",
       "  'doch',\n",
       "  'schon',\n",
       "  'lange',\n",
       "  'mit',\n",
       "  'dem',\n",
       "  'vertraut'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "from nltk.tokenize import word_tokenize\n",
    "from os.path import join\n",
    "\n",
    "path = r'..\\Data'\n",
    "\n",
    "with open(join(path, 'Heidegger_bereinigt.txt'), 'r', \n",
    "           encoding='latin-1') as doc:\n",
    "     text = doc.read()\n",
    "     text_words = word_tokenize(text)\n",
    "\n",
    "text[:100], text_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c26eb5",
   "metadata": {},
   "source": [
    "### 02 - Den *fastText*-Vektorraum laden\n",
    "**Hinweis**: Da die *fastText*-Datei mit den Vektoren (Bezeichnung: *wiki.de.align.vec*) zu groß ist, befindest sich diese nicht im Repository und *Data*. Wenn Sie den Code ausführen möchten, gehen Sie wie folgt vor:\n",
    "- Laden Sie zunächst den *fastText*-Vektorraum von folgender Seite: https://fasttext.cc/docs/en/aligned-vectors.html (Es handelt sich um den *aligned vector German (Text)*. Direkter Download Link: https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.de.align.vec [Zuletzt geprüft am 10.01.2022]\n",
    "- Platzieren Sie die Datei im Repository im Ordner *Data* (oder an einem anderen Ort auf ihrem Rechner und passen Sie dann die Variable *path* entsprechend an.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bda6ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_data(filepath: str):\n",
    "    with open(filepath, 'r', encoding='utf-8') as doc:\n",
    "        next(doc)\n",
    "        word_vec = {}\n",
    "        for row in doc:\n",
    "            row = row.split(' ')\n",
    "            word = row[0]\n",
    "            vec = np.array([float(v) for v in row[1:]])\n",
    "            word_vec[word] = vec\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949fc496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2275233, (300,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'..\\Data'\n",
    "\n",
    "word_vec = load_vector_data(join(path, 'wiki.de.align.vec'))\n",
    "len(word_vec), word_vec['angst'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38c7a82",
   "metadata": {},
   "source": [
    "### 03 - Das neuronale Netz aufsetzen\n",
    "Daraus extrahieren wir die Gewichte des Embedding-Layers, die für die unbekannten Wörter initialisiert sind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e338072b",
   "metadata": {},
   "source": [
    "#### Zunächst die Anzahl der Wörter im Heidegger-Text eruieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a67231a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['!', '#', '%', '&', '(', ')', '*', ',', '.', '.a'], 12653)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_text = [word for word in set(text_words)]\n",
    "words_in_text.sort()\n",
    "words_in_text[:10], len(words_in_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724d6332",
   "metadata": {},
   "source": [
    "#### Modell zusammenstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70c2a3a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 300)           3795900   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 300)               541800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12653)             3808553   \n",
      "=================================================================\n",
      "Total params: 8,146,253\n",
      "Trainable params: 8,146,253\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GRU, Dropout\n",
    "\n",
    "len_seq = 20\n",
    "num_words = len(words_in_text)\n",
    "num_vects = 300 # Länge der Vektoren aus fastText\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words, \n",
    "                    output_dim=num_vects, \n",
    "                    input_length=len_seq))\n",
    "model.add(GRU(units=num_vects))\n",
    "model.add(Dense(units=num_words, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(  loss='sparse_categorical_crossentropy', \n",
    "                optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7caafb",
   "metadata": {},
   "source": [
    "#### Jetzt die Gewichte des Embedding-Layers extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "144f0c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12653, 300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = model.get_layer(index=0).get_weights()[0]\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c003f0d",
   "metadata": {},
   "source": [
    "### 04 - Den Vektorraum für die Embedding-Schicht aufbauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16554ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_word_vectors(words: list, word_vect: dict, weights: np.array ):\n",
    "    word_vect_index = {}\n",
    "    word_vect_task = weights.copy()\n",
    "    count = 0\n",
    "    for word in words:\n",
    "        if word in word_vec:\n",
    "            word_vect_task[count] = word_vect[word]\n",
    "        word_vect_index[word] = count\n",
    "        count += 1\n",
    "    return word_vect_index, word_vect_task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f24ab",
   "metadata": {},
   "source": [
    "#### Jetzt die Funktion aufrufen. Dabei übergeben wir: \n",
    "- die Liste mit den im Text enthaltenen Wörtern (*words_in_test*)\n",
    "- den *fastText*-Vetkorraum (der die Vektoren aller in *fastText* verzeichneten Wörter enthält, *word_vec*)\n",
    "- die initialisierten Gewichte aus der Embedding-Layer (*weights*) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c96205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12653, (12653, 300))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vect_index, word_vect_task = all_word_vectors(\n",
    "                                words_in_text, \n",
    "                                word_vec,\n",
    "                                weights)\n",
    "len(word_vect_index), word_vect_task.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc8eb7",
   "metadata": {},
   "source": [
    "#### Jetzt die Referenz auf das fastText-Objekt kappen, um Platz im Arbeitsspeicher zu schaffen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a11d82bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775c9f5",
   "metadata": {},
   "source": [
    "### 05 - Den konstruierten Vektorraum in die Embedding-Layer einfügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0ea545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(index=0).set_weights([word_vect_task])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab98b968",
   "metadata": {},
   "source": [
    "### 06 - Die Trainingsdaten vorbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "775f206d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequences( text: list, \n",
    "                       words_index: dict, \n",
    "                       len_seq: int):\n",
    "    data_X = []\n",
    "    data_y = []\n",
    "    for i in range(len(text)-(len_seq+1)):\n",
    "        X = text[i:i+len_seq]\n",
    "        y = text[i+len_seq]\n",
    "        X = [words_index[word] for word in X]\n",
    "        y = words_index[y]\n",
    "        data_X.append(X)\n",
    "        data_y.append(y)\n",
    "    return np.array(data_X), np.array(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45a5ca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = text_to_sequences(text_words, \n",
    "                         word_vect_index,\n",
    "                         len_seq=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1360ac4d",
   "metadata": {},
   "source": [
    "#### Trainings- und Testdaten separieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62b36f4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((157311, 20), (17480, 20))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, random_state=12)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f034ed8e",
   "metadata": {},
   "source": [
    "### 07 - Das Modell anlernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4a26b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1229/1229 [==============================] - 365s 297ms/step - loss: 6.0979 - accuracy: 0.1080 - val_loss: 5.5061 - val_accuracy: 0.1516\n",
      "Epoch 2/100\n",
      "1229/1229 [==============================] - 376s 306ms/step - loss: 5.0342 - accuracy: 0.1758 - val_loss: 5.2738 - val_accuracy: 0.1805\n",
      "Epoch 3/100\n",
      "1229/1229 [==============================] - 387s 315ms/step - loss: 4.4170 - accuracy: 0.2143 - val_loss: 5.2504 - val_accuracy: 0.1871\n",
      "Epoch 4/100\n",
      "1229/1229 [==============================] - 349s 284ms/step - loss: 3.8226 - accuracy: 0.2589 - val_loss: 5.3415 - val_accuracy: 0.1899\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "checkpoint = ModelCheckpoint( filepath='heidegger_fasttext_no_missings.h5',\n",
    "                              save_best_only=False)\n",
    "stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=128,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[checkpoint, stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e7023",
   "metadata": {},
   "source": [
    "### 08 - Schätzungen durchführen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada59e22",
   "metadata": {},
   "source": [
    "#### Die Schätzfunktion erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4c1854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def generate_phrases(model, \n",
    "                    words_index: dict,\n",
    "                    starter: str,\n",
    "                    len_seq = 20, \n",
    "                    num_words=100):\n",
    "    index_words = dict([(idx, keys) for keys, idx in words_index.items()])\n",
    "    starter = word_tokenize(starter.lower())\n",
    "    print(' '.join(starter))\n",
    "    starter = np.array([words_index[word] for word \n",
    "                        in starter]).reshape(1, -1)\n",
    "    for i in range(num_words):\n",
    "        word_probs = model.predict(starter)[0]\n",
    "        word_pred = np.argmax(word_probs)\n",
    "        word = index_words[word_pred]\n",
    "        print(word, end=' ')\n",
    "        starter = starter[0].tolist()[1:]\n",
    "        starter.append(words_index[word])\n",
    "        starter = np.array(starter).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7adbeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wie unterscheidet sich das , wovor die angst sich ängstet , von dem , wovor die furcht sich fürchtet ?\n",
      "das man ist das gerede , das heißt das , was es ist , ist es , das heißt , was es ist , und zwar , daß es sich in seinem sein um sein sein kann . das sein des seienden ist wesenhaft das sein des daseins . das sein des seienden ist wesenhaft das sein des daseins . das sein des seienden , das wir nennen , ist das sein des daseins . das sein des seienden ist wesenhaft das sein des daseins . das sein des seienden , das wir nennen , ist das sein des daseins "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('heidegger_fasttext_no_missings.h5')\n",
    "\n",
    "starter = '''wie unterscheidet sich das, wovor die angst sich ängstet, von dem, wovor die furcht sich fürchtet?'''\n",
    "\n",
    "generate_phrases( model,\n",
    "                  word_vect_index,  \n",
    "                  starter,\n",
    "                  num_words=100 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
