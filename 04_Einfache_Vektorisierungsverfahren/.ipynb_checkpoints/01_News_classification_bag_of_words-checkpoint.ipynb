{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa02a1d",
   "metadata": {},
   "source": [
    "# 4.4.1\tVektorisierung mit dem Count-Vectorizer (German-News-Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bb52cc",
   "metadata": {},
   "source": [
    "### 01 - Daten laden und inspizieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51fd4c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text        tag1\n",
      "0  Anzeige\\n\\nDie Corona-Krise hat auch vor dem e...  Wirtschaft\n",
      "1  Deutschland kassiert in der fünften Minute der...       Sport\n",
      "2  An der Great Northern Road schrauben Monteure ...  Wirtschaft\n",
      "3  Zwischen Ende Juni und Ende September hat sich...  Wirtschaft\n",
      "4  Lange Zeit hatten Politiker für innerländische...  Wirtschaft\n",
      "\n",
      "Verteilung auf die Rubriken:\n",
      "Politik       9680\n",
      "Sport         5227\n",
      "Wirtschaft    3990\n",
      "Name: tag1, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from os.path import join\n",
    "\n",
    "path = r'..\\Data'\n",
    "\n",
    "df = pd.read_csv(join(path, 'german_news_psw.csv'))\n",
    "print(df.head())\n",
    "print()\n",
    "print('Verteilung auf die Rubriken:\\n', df.tag1.value_counts(), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7f01c6",
   "metadata": {},
   "source": [
    "### Durchschnittliche Anzahl der Wörter pro Dokument berechnen\n",
    "***Bitte beachten***: Vor der ersten Benutzung von *nltk* müssen Sie ggf. mit dem Befehl *nltk.download()* \n",
    "einige Basis-Pakte herunterladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7563186a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9a46b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(557.2310948827857, 18897)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Text-Spalte in Liste konvertieren\n",
    "text = df['text'].values.tolist()\n",
    "\n",
    "i = 0\n",
    "num_words = 0\n",
    "for doc in text:\n",
    "    i += 1\n",
    "    num_words += len(word_tokenize(doc))\n",
    "average = num_words / i \n",
    "average, len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a4194c",
   "metadata": {},
   "source": [
    "### 02 - X- und y-Variablen separieren und Trainings- und Testpartitionen sampeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e8b2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15117,), (3780,), (15117,), (3780,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['text']\n",
    "y = df['tag1']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                    test_size=.2, random_state=11)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673095d",
   "metadata": {},
   "source": [
    "## 03 - Bag-of-words aus den x-Daten mit *sklearn.CountVectorizer* erzeugen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb58d11f",
   "metadata": {},
   "source": [
    "#### Zunächst die Stopwords laden (liegen im Verzeichnis *..\\Data* als txt-Datei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e8e4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0\n",
      "0     a\n",
      "1    ab\n",
      "2  aber\n",
      "3   ach\n",
      "4  acht\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a', 'ab', 'aber', 'ach', 'acht', 'achte', 'achten', 'achter', 'achtes', 'ag']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw_df = pd.read_csv(join(path, 'stopwords_de.txt'), header=None)\n",
    "print(sw_df.head())\n",
    "stopwords = sw_df[0].values.tolist() ## Stopwords in Liste überführen\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2154a5",
   "metadata": {},
   "source": [
    "#### Jetzt den *Bag-of-words* mit *CountVectorizer* erstellen \n",
    "- *min_df = min document frequency* (When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold., vgl. https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html ) \n",
    "- Stopwords werden bei der Instanziierung von CountVectorizer übergeben und aus der Analyse/Erzeugung des *Bag-of-words* ausgeschlossen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85453b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30101"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(stop_words=stopwords, min_df=10)\n",
    "cv.fit(X_train)\n",
    "X_train_cv = cv.transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)\n",
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e399d28",
   "metadata": {},
   "source": [
    "### 04 - Softmax-Regression instanziieren und Training über vorbereitete Daten durchführen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d227c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial', max_iter=10000)\n",
    "model.fit(X_train_cv, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0872b3d",
   "metadata": {},
   "source": [
    "### 05 - Überprüfung der Qualität des Modells (Testdaten verwenden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b5e0a",
   "metadata": {},
   "source": [
    "#### Testweise eine Schätzung durchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf5647da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Politik'], dtype=object),\n",
       " array([[0.9  , 0.005, 0.095]]),\n",
       " array(['Politik', 'Sport', 'Wirtschaft'], dtype=object))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Rohtext\n",
    "x_pred =  r'''In den Streit über die Einführung einer Impfpflicht in Deutschland hat sich jetzt \n",
    "              auch der frühere Bundesverkehrsminister Andreas Scheuer eingeschaltet. \n",
    "            Der CSU-Politiker fordert Kanzler Olaf Scholz (SPD) auf, im Bundestag die \n",
    "            Vertrauensfrage zu stellen.'''\n",
    "\n",
    "## Umwandlung in Bag-of-words\n",
    "x_bag = cv.transform([x_pred])\n",
    "\n",
    "## Schätzungen mit dem Modell\n",
    "y_pred = model.predict(x_bag) # label schätzen\n",
    "y_pred_proba = model.predict_proba(x_bag) # wahrscheinlichkeiten für alle labels ausgeben\n",
    "\n",
    "y_pred, y_pred_proba.round(3), model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea99769a",
   "metadata": {},
   "source": [
    "#### Accuracy und Confusion-Matrix erstellen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ce660d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.949\n",
      "\n",
      "Confusion-Matrix:\n",
      "[[1865    8   70]\n",
      " [  13 1022    1]\n",
      " [  96    6  699]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Politik', 'Sport', 'Wirtschaft'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Schätzungen des Modells für die Testdaten erzeugen\n",
    "y_test_pred = model.predict(X_test_cv) # die Testdaten wurden bereits weiter oben in Bag-of-words umgewandelt\n",
    "\n",
    "# Vergleich der vom Modell geschätzten y-Werten mit den tatsächlichen y-Werten\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Ausgabe\n",
    "print('Accuray: ', acc.round(3), '\\n\\nConfusion-Matrix:\\n', matrix, sep='')\n",
    "model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84cbbd5",
   "metadata": {},
   "source": [
    "### 06 - Mit TfIdf-Vectorizer arbeiten\n",
    "Wir verwenden die gleichen Daten wie oben, fürhen die Bag-of-words-Umwandlungen aber mit einem *TfidfVectorizer* durch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a9282",
   "metadata": {},
   "source": [
    "#### a) Tfidf-Vektorisierer anlernen und Daten umwandeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "642fed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords, min_df=10)\n",
    "tfidf.fit(X_train)\n",
    "X_train_tfidf = tfidf.transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2e38ad",
   "metadata": {},
   "source": [
    "#### b) Softmax-Modell mit vektorisierten Daten anlernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1590143f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial', max_iter=10000)\n",
    "model.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ec74ea",
   "metadata": {},
   "source": [
    "#### Qualität des Modells überprüfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eea837fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray: 0.952\n",
      "\n",
      "Confusion-Matrix:\n",
      "[[1883    3   57]\n",
      " [  15 1020    1]\n",
      " [  98    6  697]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Politik', 'Sport', 'Wirtschaft'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test_tfidf)\n",
    "\n",
    "acc = accuracy_score(y_test, y_test_pred)\n",
    "matrix = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Ausgabe\n",
    "print('Accuray: ', acc.round(3), '\\n\\nConfusion-Matrix:\\n', matrix, sep='')\n",
    "model.classes_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
