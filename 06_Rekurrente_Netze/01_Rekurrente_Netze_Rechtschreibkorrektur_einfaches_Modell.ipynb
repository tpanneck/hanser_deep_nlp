{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f04b4d",
   "metadata": {},
   "source": [
    "# 6.3\tPraxis rekurrenter Netze: eine automatische Rechtschreibkorrektur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f25f85",
   "metadata": {},
   "source": [
    "### 01 - Rechtschreibfehler aus einer Liste korrekter Wörter produzieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca610c8",
   "metadata": {},
   "source": [
    "#### Daten mit korrekten Wörtern laden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71cea450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 word form  frequence\n",
      "0           1       die  527056159\n",
      "1           2       und  488790440\n",
      "2           3       der  477357554\n",
      "3           4        in  267256433\n",
      "4           5       das  201678723\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    die\n",
       "1    und\n",
       "2    der\n",
       "3     in\n",
       "4    das\n",
       "Name: word form, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os.path import join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "path = r'..\\Data'\n",
    "file = 'german-word-list-total.csv'\n",
    "\n",
    "df = pd.read_csv(join(path, file), sep='\\t')\n",
    "print(df.head())\n",
    "### Wörter in Kleinbuchstaben umwandeln\n",
    "y = df['word form'].map(lambda x: x.lower())\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68660d39",
   "metadata": {},
   "source": [
    "#### Klasse zur Produktion von Rechtschreibfehlern "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b79f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateSpellingMistakes:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.alphabet = [word for word in 'abcdefghijklmnopqrstuvwxyzßäöü']\n",
    "    \n",
    "    def gen_misspellings_as_df(self, y: np.array):\n",
    "        spelling_data = self.__gen_misspellings(y)\n",
    "        df_spelling = pd.DataFrame(spelling_data)\n",
    "        df_spelling.columns = ['misspelling', 'word']\n",
    "        return df_spelling\n",
    "\n",
    "    def __gen_misspellings(self, y: np.array):\n",
    "        spelling_data = []\n",
    "        for word in y:\n",
    "            for i in range(len(word)*len(word)):\n",
    "                rand_num = np.random.randint(0,6)\n",
    "                if rand_num == 0:\n",
    "                    mistake = self.__random_char_exchange(word, add=True)\n",
    "                if rand_num==1 or rand_num==2:\n",
    "                    mistake = self.__char_dublication(word)\n",
    "                else:\n",
    "                    mistake = self.__random_char_exchange(word)\n",
    "                if len(word) > 5 and np.random.randint(0,10) == 0:\n",
    "                    mistake = self.__random_char_exchange(mistake)       \n",
    "                spelling_data.append((mistake, word))\n",
    "        return spelling_data\n",
    "\n",
    "    def __char_dublication(self, word):\n",
    "        idx = np.random.randint(0,len(word))\n",
    "        char = word[idx]\n",
    "        if idx==len(word)-1:\n",
    "            mistake = word + char\n",
    "        else:\n",
    "            mistake = word[:idx] + char + word[idx:]\n",
    "        return mistake\n",
    "\n",
    "    def __random_char_exchange(self, word, add=False):\n",
    "        char = self.alphabet[np.random.randint(0,len(self.alphabet))]\n",
    "        idx = np.random.randint(0,len(word))\n",
    "        if not add:\n",
    "            if idx==len(word)-1:\n",
    "                mistake = word[:idx] + char\n",
    "            else:\n",
    "                mistake = word[:idx] + char + word[idx+1:]\n",
    "        else:\n",
    "            if idx==len(word)-1:\n",
    "                mistake = word + char\n",
    "            else:\n",
    "                mistake = word[:idx] + char + word[idx:]\n",
    "        return mistake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef231c47",
   "metadata": {},
   "source": [
    "#### Fehlerhafte aus korrekten Wörtern produzieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb789631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15916    werrt\n",
       " 15917     wcrt\n",
       " 15918     lert\n",
       " 15919     gert\n",
       " 15920     werd\n",
       " Name: misspelling, dtype: object,\n",
       " 15916    wert\n",
       " 15917    wert\n",
       " 15918    wert\n",
       " 15919    wert\n",
       " 15920    wert\n",
       " Name: word, dtype: object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spelling = CreateSpellingMistakes().gen_misspellings_as_df(y.values)\n",
    "misspelling, word = df_spelling['misspelling'], df_spelling['word']\n",
    "misspelling[-5:], word[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d5cee3",
   "metadata": {},
   "source": [
    "## 02 - Daten encodieren "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b2c6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Funktion für Encodierung erzeugen: SequenceEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76a76817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class SequenceEncoder:\n",
    "    \n",
    "    def __init__( self, x: np.array, \n",
    "                        y: np.array, \n",
    "                        filler='\\t'):\n",
    "        if len(y) != len(x):\n",
    "            raise ValueError('x and y must have same length!')\n",
    "        self.y = y\n",
    "        self.x = x\n",
    "        self.filler = filler\n",
    "        self.__variables_one_hot_encoding()\n",
    "        self.__variables_target()\n",
    "\n",
    "    def gen_feature_target_data(self):\n",
    "        X = self.gen_one_hot_data(self.x)\n",
    "        y = np.array([self.word_to_int(word) for word in self.y])\n",
    "        return X, y\n",
    "    \n",
    "    def gen_one_hot_data(self, data: np.array):\n",
    "        one_hot_data = np.zeros(shape=(len(data), \n",
    "                                self.max_word_length, \n",
    "                                len(self.char_idx)))\n",
    "        for idx, word in enumerate(data):\n",
    "            for i, char in enumerate(word):\n",
    "                if char in self.char_idx:\n",
    "                    one_hot_data[idx, i, self.char_idx[char]] = 1\n",
    "            for i in range(len(word), self.max_word_length):\n",
    "                one_hot_data[idx, i, self.char_idx[self.filler]] = 1\n",
    "        return one_hot_data\n",
    "\n",
    "    def one_hot_to_word(self, one_hot: np.array):\n",
    "        word = []\n",
    "        for col in one_hot:\n",
    "            idx = np.argmax(col)\n",
    "            word.append(self.idx_char[idx])\n",
    "        return ''.join(word)\n",
    "    \n",
    "    def int_to_word(self, word_int: int):\n",
    "        return self.idx_word[word_int]\n",
    "\n",
    "    def word_to_int(self, word: str):\n",
    "        return self.word_idx[word]\n",
    "\n",
    "    def __variables_one_hot_encoding(self):\n",
    "        self.max_word_length = len(max(self.x, key=len))\n",
    "        self.__gen_charset()\n",
    "        self.char_idx = dict([(char, idx) for idx, char in enumerate(self.charset)])\n",
    "        self.idx_char = dict([(idx, char) for idx, char in enumerate(self.charset)])\n",
    "\n",
    "    def __variables_target(self):\n",
    "        target_word_list = sorted(list(set(self.y.tolist())))\n",
    "        self.word_idx = dict([word, i] for i, word in enumerate(target_word_list))\n",
    "        self.idx_word = dict([i, word] for i, word in enumerate(target_word_list))\n",
    "\n",
    "    def __gen_charset(self):\n",
    "        charset = set()\n",
    "        for word in self.x:\n",
    "            for char in word:\n",
    "                charset.add(char)\n",
    "        charset = list(charset)\n",
    "        charset.sort()\n",
    "        if self.filler in charset:\n",
    "            raise ValueError('x contains filler!')\n",
    "        else:\n",
    "            charset.append(self.filler)\n",
    "        self.charset = charset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f10c1af",
   "metadata": {},
   "source": [
    "#### Objekt von SeqenceEncoder erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5970e114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ß', 'ä', 'ö', 'ü', '.', '\\t'] \n",
      " <__main__.SequenceEncoder object at 0x00000208C35EA188>\n"
     ]
    }
   ],
   "source": [
    "import joblib \n",
    "\n",
    "### a) Dictionary aus Buchstaben des Alphabets + Trennzeichen erzeugen\n",
    "charset = 'abcdefghijklmnopqrstuvwxyzßäöü.\\t'\n",
    "charset = [char for char in charset]\n",
    "\n",
    "### b) SequenceEncoder-Objekt instanziieren\n",
    "seq_encoder = SequenceEncoder(df_spelling['misspelling'].values, \n",
    "            df_spelling['word'].values)\n",
    "print(charset, '\\n', seq_encoder)\n",
    "\n",
    "### c) Serialisieurng des SequenceEncoder-Objekts \n",
    "joblib.dump(seq_encoder, 'seq_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9374fb39",
   "metadata": {},
   "source": [
    "#### Mit SequenceEncoder X, y - Trainingsdaten erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb138875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15921, 15, 32), (15921,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = seq_encoder.gen_feature_target_data()\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcfc830",
   "metadata": {},
   "source": [
    "#### Überprüfen, ob SequenceEncoder funktioniert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f91c9a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('diee\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t', 'die')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "misspelling = seq_encoder.one_hot_to_word(X[0])\n",
    "word = seq_encoder.int_to_word(y[0])\n",
    "misspelling, word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e248e",
   "metadata": {},
   "source": [
    "#### Train-/Test-Split erzeugen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "643692ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12736, 15, 32), (12736,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                    test_size=.2, random_state=11)\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56563e09",
   "metadata": {},
   "source": [
    "## 03 - Rekurrentes Modell mit TensorFlow aufbauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c698efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gru (GRU)                    (None, 64)                18816     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               32500     \n",
      "=================================================================\n",
      "Total params: 51,316\n",
      "Trainable params: 51,316\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, GRU\n",
    "\n",
    "seq_length = seq_encoder.max_word_length # =15\n",
    "num_chars = len(seq_encoder.charset) # =32\n",
    "num_words = len(seq_encoder.word_idx) # =500\n",
    "\n",
    "model = Sequential()\n",
    "model.add(GRU(units=num_chars*2,\n",
    "              input_shape=(seq_length, num_chars)))\n",
    "model.add(Dense(units=num_words, activation='softmax'))\n",
    "\n",
    "model.compile(  loss='sparse_categorical_crossentropy', \n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81614d74",
   "metadata": {},
   "source": [
    "## 04 - Training mit Callbacks starten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1eb96009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "398/398 [==============================] - 3s 7ms/step - loss: 5.4819 - accuracy: 0.0217 - val_loss: 4.9788 - val_accuracy: 0.0355\n",
      "Epoch 2/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 4.7827 - accuracy: 0.0383 - val_loss: 4.7411 - val_accuracy: 0.0411\n",
      "Epoch 3/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 4.6028 - accuracy: 0.0507 - val_loss: 4.6291 - val_accuracy: 0.0414\n",
      "Epoch 4/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 4.4910 - accuracy: 0.0577 - val_loss: 4.5196 - val_accuracy: 0.0581\n",
      "Epoch 5/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 4.4083 - accuracy: 0.0689 - val_loss: 4.4697 - val_accuracy: 0.0710\n",
      "Epoch 6/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 4.3301 - accuracy: 0.0813 - val_loss: 4.3552 - val_accuracy: 0.0738\n",
      "Epoch 7/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 4.2094 - accuracy: 0.1029 - val_loss: 4.2318 - val_accuracy: 0.1020\n",
      "Epoch 8/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 4.0418 - accuracy: 0.1310 - val_loss: 4.0399 - val_accuracy: 0.1366\n",
      "Epoch 9/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 3.8314 - accuracy: 0.1614 - val_loss: 3.8231 - val_accuracy: 0.1683\n",
      "Epoch 10/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 3.6137 - accuracy: 0.1907 - val_loss: 3.6393 - val_accuracy: 0.1972\n",
      "Epoch 11/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 3.4383 - accuracy: 0.2162 - val_loss: 3.4967 - val_accuracy: 0.2154\n",
      "Epoch 12/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 3.2650 - accuracy: 0.2487 - val_loss: 3.3328 - val_accuracy: 0.2320\n",
      "Epoch 13/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 3.1021 - accuracy: 0.2808 - val_loss: 3.1620 - val_accuracy: 0.2779\n",
      "Epoch 14/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 2.9533 - accuracy: 0.3065 - val_loss: 3.0908 - val_accuracy: 0.2732\n",
      "Epoch 15/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 2.8119 - accuracy: 0.3360 - val_loss: 2.9164 - val_accuracy: 0.3237\n",
      "Epoch 16/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 2.6688 - accuracy: 0.3632 - val_loss: 2.7698 - val_accuracy: 0.3469\n",
      "Epoch 17/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 2.5197 - accuracy: 0.3938 - val_loss: 2.6491 - val_accuracy: 0.3705\n",
      "Epoch 18/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 2.3858 - accuracy: 0.4238 - val_loss: 2.5378 - val_accuracy: 0.3950\n",
      "Epoch 19/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 2.2626 - accuracy: 0.4494 - val_loss: 2.3832 - val_accuracy: 0.4248\n",
      "Epoch 20/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 2.1366 - accuracy: 0.4790 - val_loss: 2.2746 - val_accuracy: 0.4518\n",
      "Epoch 21/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 2.0152 - accuracy: 0.5069 - val_loss: 2.1686 - val_accuracy: 0.4716\n",
      "Epoch 22/200\n",
      "398/398 [==============================] - 3s 7ms/step - loss: 1.8985 - accuracy: 0.5349 - val_loss: 2.0529 - val_accuracy: 0.5058\n",
      "Epoch 23/200\n",
      "398/398 [==============================] - 3s 7ms/step - loss: 1.7858 - accuracy: 0.5583 - val_loss: 1.9857 - val_accuracy: 0.5272\n",
      "Epoch 24/200\n",
      "398/398 [==============================] - 3s 8ms/step - loss: 1.6826 - accuracy: 0.5900 - val_loss: 1.8547 - val_accuracy: 0.5435\n",
      "Epoch 25/200\n",
      "398/398 [==============================] - 3s 7ms/step - loss: 1.5953 - accuracy: 0.6068 - val_loss: 1.7655 - val_accuracy: 0.5739\n",
      "Epoch 26/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 1.4972 - accuracy: 0.6312 - val_loss: 1.6857 - val_accuracy: 0.5931\n",
      "Epoch 27/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 1.4163 - accuracy: 0.6517 - val_loss: 1.6342 - val_accuracy: 0.5956\n",
      "Epoch 28/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 1.3380 - accuracy: 0.6712 - val_loss: 1.5518 - val_accuracy: 0.6214\n",
      "Epoch 29/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 1.2648 - accuracy: 0.6858 - val_loss: 1.4942 - val_accuracy: 0.6352\n",
      "Epoch 30/200\n",
      "398/398 [==============================] - 3s 7ms/step - loss: 1.2118 - accuracy: 0.6996 - val_loss: 1.4575 - val_accuracy: 0.6471\n",
      "Epoch 31/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 1.1367 - accuracy: 0.7184 - val_loss: 1.3895 - val_accuracy: 0.6675\n",
      "Epoch 32/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 1.0819 - accuracy: 0.7337 - val_loss: 1.3502 - val_accuracy: 0.6669\n",
      "Epoch 33/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 1.0305 - accuracy: 0.7418 - val_loss: 1.3197 - val_accuracy: 0.6722\n",
      "Epoch 34/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 0.9867 - accuracy: 0.7522 - val_loss: 1.2827 - val_accuracy: 0.6785\n",
      "Epoch 35/200\n",
      "398/398 [==============================] - 3s 7ms/step - loss: 0.9268 - accuracy: 0.7688 - val_loss: 1.2264 - val_accuracy: 0.6998\n",
      "Epoch 36/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 0.8818 - accuracy: 0.7817 - val_loss: 1.2082 - val_accuracy: 0.7011\n",
      "Epoch 37/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 0.8402 - accuracy: 0.7914 - val_loss: 1.1713 - val_accuracy: 0.7140\n",
      "Epoch 38/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 0.8006 - accuracy: 0.8052 - val_loss: 1.1559 - val_accuracy: 0.7061\n",
      "Epoch 39/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 0.7641 - accuracy: 0.8086 - val_loss: 1.0931 - val_accuracy: 0.7350\n",
      "Epoch 40/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 0.7317 - accuracy: 0.8196 - val_loss: 1.0882 - val_accuracy: 0.7297\n",
      "Epoch 41/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 0.6856 - accuracy: 0.8310 - val_loss: 1.0330 - val_accuracy: 0.7482\n",
      "Epoch 42/200\n",
      "398/398 [==============================] - 3s 6ms/step - loss: 0.6529 - accuracy: 0.8374 - val_loss: 1.0254 - val_accuracy: 0.7463\n",
      "Epoch 43/200\n",
      "(...)398/398 [==============================] - 2s 6ms/step - loss: 0.1598 - accuracy: 0.9656 - val_loss: 0.7917 - val_accuracy: 0.8192\n",
      "Epoch 75/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 0.1535 - accuracy: 0.9658 - val_loss: 0.7774 - val_accuracy: 0.8248\n",
      "Epoch 76/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 0.1423 - accuracy: 0.9702 - val_loss: 0.8001 - val_accuracy: 0.8157\n",
      "Epoch 77/200\n",
      "398/398 [==============================] - 2s 6ms/step - loss: 0.1487 - accuracy: 0.9662 - val_loss: 0.7987 - val_accuracy: 0.8160\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "stopping = EarlyStopping( monitor='val_loss', \n",
    "                          patience=10,\n",
    "                        restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint( filepath='model_auto_correction_1.h5',\n",
    "                              monitor='val_loss',\n",
    "                              save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=32,\n",
    "            callbacks=[stopping, checkpoint],\n",
    "            validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586e19f0",
   "metadata": {},
   "source": [
    "## 05 - Schätzungen durchführen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa2fd3b",
   "metadata": {},
   "source": [
    "#### Modell nach dem Training wieder laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd594b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('model_auto_correction_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42461ec",
   "metadata": {},
   "source": [
    "#### Schätzungen mit falschen Wörtern durchführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "961e7b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allein\n",
      "frage\n",
      "beispiel\n"
     ]
    }
   ],
   "source": [
    "test_example = np.array(['alllein', 'frahe', 'beistiel'])\n",
    "test_example = seq_encoder.gen_one_hot_data(test_example)\n",
    "\n",
    "pred_word_prob = model.predict(test_example)\n",
    "pred_word_idx = np.argmax(pred_word_prob, axis=1)\n",
    "\n",
    "### 3) Daten decodieren\n",
    "for idx in pred_word_idx:\n",
    "    print(seq_encoder.int_to_word(idx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
